{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/Shakilkhan24/Playground_DL/blob/main/rag_pipeline_langchain.ipynb",
      "authorship_tag": "ABX9TyO2uISkhTzEBUQW79ytKSp2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shakilkhan24/Playground_DL/blob/main/rag_pipeline_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-openai"
      ],
      "metadata": {
        "id": "4w_ibz7VFnzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "def bfs_crawl(url, limit):\n",
        "    visited = set()\n",
        "    queue = [url]\n",
        "    count = 0\n",
        "    extracted_links = []\n",
        "\n",
        "    while queue and count < limit:\n",
        "        current_url = queue.pop(0)\n",
        "        visited.add(current_url)\n",
        "\n",
        "        try:\n",
        "            response = requests.get(current_url)\n",
        "        except requests.exceptions.RequestException:\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        for link in soup.find_all('a'):\n",
        "            href = link.get('href')\n",
        "            if href:\n",
        "                absolute_url = urljoin(current_url, href)\n",
        "                parsed_url = urlparse(absolute_url)\n",
        "\n",
        "                if parsed_url.netloc == urlparse(url).netloc:\n",
        "                    if absolute_url not in visited and absolute_url not in queue:\n",
        "                        queue.append(absolute_url)\n",
        "                        visited.add(absolute_url)\n",
        "                        count += 1\n",
        "                        extracted_links.append(absolute_url)\n",
        "\n",
        "        print(f\"Crawled: {current_url}\")\n",
        "\n",
        "    return extracted_links\n",
        "\n",
        "\n",
        "start_url = \"https://python.langchain.com/docs/get_started/introduction\"\n",
        "extraction_limit = 500\n",
        "links = bfs_crawl(start_url, extraction_limit)\n"
      ],
      "metadata": {
        "id": "o28pNdADyCsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
        "import requests\n",
        "\n",
        "total_info = []\n",
        "\n",
        "for i, link in enumerate(links):\n",
        "    html = requests.get(link).text\n",
        "    with open(f'sky_{i}.html', 'w') as f:\n",
        "        f.write(html)\n",
        "\n",
        "    loader = UnstructuredHTMLLoader(f'sky_{i}.html')\n",
        "    extracted_info = loader.load()\n",
        "    total_info.extend(extracted_info)\n",
        "\n"
      ],
      "metadata": {
        "id": "Gr3N4pqM2-eB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_information=''\n",
        "for info in total_info:\n",
        "  total_information +=(info.page_content)\n",
        "  total_information+='\\n'\n",
        "with open('final_total.txt', 'w') as f:\n",
        "    f.write(total_information)\n"
      ],
      "metadata": {
        "id": "j6-m8sH46d8O"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i-gus14i-YPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# datas we have , but anyhow it is not showing"
      ],
      "metadata": {
        "id": "KX_OYuIn-Y7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/total.txt', 'r') as f:\n",
        "    contents = f.read()\n",
        "    print(contents)\n"
      ],
      "metadata": {
        "id": "YCuEziTB7ibh",
        "outputId": "885f07a0-288d-44d2-8e4e-ed97bf529a51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unstructured"
      ],
      "metadata": {
        "id": "XCr_zDXIHH2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V-4I8g5aS6r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_key='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'\n"
      ],
      "metadata": {
        "id": "mxGHXQNURh-h"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "embedd=OpenAIEmbeddings(openai_api_key=api_key)"
      ],
      "metadata": {
        "id": "8mLZ6MAbOUo8",
        "outputId": "10a03100-fc48-4584-e6fe-44e757a4bec4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  warn_deprecated(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(embed_vec)\n",
        "# print(len(embed_vec))"
      ],
      "metadata": {
        "id": "PyNJhXIxRNfX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "id": "x-wjfGP4Uivg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# chat with any webpage (given url)"
      ],
      "metadata": {
        "id": "N2m8vdfS_XBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import requests\n",
        "import chromadb\n",
        "from typing import List, Dict\n",
        "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAI\n",
        "from langchain import PromptTemplate\n",
        "url='https://huggingface.co/learn/nlp-course/chapter3/4'\n",
        "html = requests.get(url).text\n",
        "with open('explore.html', 'w') as f:\n",
        "    f.write(html)\n",
        "\n",
        "chat = ChatOpenAI(temperature=0, openai_api_key=api_key)\n",
        "embedding = OpenAIEmbeddings(openai_api_key=api_key)\n",
        "\n",
        "loader = UnstructuredHTMLLoader(\"explore.html\")\n",
        "docs = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        " )\n",
        "\n",
        "text_chunks = splitter.split_documents(docs)\n",
        "\n",
        "db = Chroma.from_documents(text_chunks, embedding)\n",
        "chat_model = ChatOpenAI(openai_api_key=api_key)\n",
        "\n",
        "PROMPTTEMPLATE =\"\"\"\n",
        "Answer the question based only on the following context:\n",
        "\n",
        "{context}\n",
        "\n",
        "---\n",
        "\n",
        "Answer the question based on the above context: {question}\n",
        "\"\"\"\n",
        "def ask_question_from_webpage(query: str):\n",
        "    def ask_question(query):\n",
        "        docs = db.similarity_search(query)\n",
        "        search_results = db.similarity_search(query, k=5)\n",
        "\n",
        "        relevant_docs = docs[0].page_content\n",
        "        prompt=PromptTemplate.from_template(PROMPTTEMPLATE)\n",
        "        prompt=prompt.format(context=relevant_docs,question=query)\n",
        "        answer = chat_model.predict(prompt)\n",
        "        return answer\n",
        "\n",
        "    prediction = ask_question(query)\n",
        "    return prediction\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-DrHrV_9jhUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# connect any text file to the openai chat model\n",
        "# so accuracy depends on [similarity_search_accuracy and txt file format]\n",
        "\n",
        "# we can connect to any book or research paper or other things for any query"
      ],
      "metadata": {
        "id": "b3QQwHQtL7OH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import requests\n",
        "import chromadb\n",
        "from typing import List, Dict\n",
        "from langchain import PromptTemplate\n",
        "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "chat = ChatOpenAI(temperature=0, openai_api_key=api_key)\n",
        "embedding = OpenAIEmbeddings(openai_api_key=api_key)\n",
        "\n",
        "loader = UnstructuredHTMLLoader(\"/content/total.txt\")\n",
        "docs = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=100,    # chunk size should be smaller , as well as chunk_overlap due to embedd. excedding criterias\n",
        "    chunk_overlap=20,\n",
        " )\n",
        "\n",
        "text_chunks = splitter.split_documents(docs)\n",
        "dbb= Chroma.from_documents(text_chunks, embedding)\n",
        "chat_model = ChatOpenAI(openai_api_key=api_key)\n",
        "\n",
        "PROMPTTEMPLATE =\"\"\"\n",
        "Answer the question based only on the following context:\n",
        "\n",
        "{context}\n",
        "\n",
        "---\n",
        "\n",
        "Answer the question based on the above context: {question}\n",
        "\"\"\"\n",
        "def ask_question_from_webpage(query: str):\n",
        "    def ask_question(query):\n",
        "        docs = dbb.similarity_search(query)\n",
        "        search_results = db.similarity_search(query, k=5)\n",
        "\n",
        "        relevant_docs = docs[0].page_content\n",
        "        prompt=PromptTemplate.from_template(PROMPTTEMPLATE)\n",
        "        prompt=prompt.format(context=relevant_docs,question=query)\n",
        "        answer = chat_model.predict(prompt)\n",
        "        return answer\n",
        "\n",
        "    prediction = ask_question(query)\n",
        "    return prediction\n"
      ],
      "metadata": {
        "id": "NHEPxQjg_J9S",
        "outputId": "1b1a268e-e98a-4480-a4c1-37dc9fdc9d0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
            "  warn_deprecated(\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# thrown on 4xx and 5xx status code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/embeddings'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-4fa7f3404c9a>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtext_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mdbb\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mChroma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mchat_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopenai_api_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36mfrom_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0mmetadatas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m         return cls.from_texts(\n\u001b[0m\u001b[1;32m    779\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m             \u001b[0membedding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36mfrom_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             ):\n\u001b[0;32m--> 736\u001b[0;31m                 chroma_collection.add_texts(\n\u001b[0m\u001b[1;32m    737\u001b[0m                     \u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m                     \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/vectorstores/chroma.py\u001b[0m in \u001b[0;36madd_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m             \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;31m# fill metadatas with empty dicts if somebody\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/embeddings/openai.py\u001b[0m in \u001b[0;36membed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m#       than the maximum context and use length-safe embedding function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mengine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeployment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_len_safe_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m     async def aembed_documents(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/embeddings/openai.py\u001b[0m in \u001b[0;36m_get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0mbatched_embeddings\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m             response = embed_with_retry(\n\u001b[0m\u001b[1;32m    495\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_chunk_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/embeddings/openai.py\u001b[0m in \u001b[0;36membed_with_retry\u001b[0;34m(embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;34m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_openai_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mretry_decorator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_retry_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/embeddings.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;34m\"/embeddings\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaybe_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_create_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbeddingCreateParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1198\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         )\n\u001b[0;32m-> 1200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 889\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    890\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m    966\u001b[0m                     \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;31m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;31m# different thread if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         return self._request(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tbXbqdafBMpL",
        "outputId": "5d9a74ad-0bcc-47dc-caf2-df5e94f7ca3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask_question_from_webpage('')"
      ],
      "metadata": {
        "id": "Rakg9_euAb5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# final **RAG** Pipeline code:\n",
        "\n",
        "\n",
        "```\n",
        "# yet to add ***callbacks, tools, agents*** and something more\n",
        "```\n",
        "\n",
        "# Very powerfull RAG system with OPENAI model"
      ],
      "metadata": {
        "id": "11EJSL26eWPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ask_question_from_webpage('can you write some code snippets how to fine tune')"
      ],
      "metadata": {
        "id": "sE1kfLNQvIE_",
        "outputId": "45e2c890-d52a-4f41-e180-c3b07dd68b60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Sure! Here are some code snippets to fine-tune a model using the Trainer API:\\n\\n```python\\nfrom transformers import Trainer, TrainingArguments\\n\\n# Define your TrainingArguments\\ntraining_args = TrainingArguments(\\n    output_dir='./results',          # output directory\\n    num_train_epochs=3,              # number of training epochs\\n    per_device_train_batch_size=8,   # batch size for training\\n    per_device_eval_batch_size=8,    # batch size for evaluation\\n    logging_dir='./logs',            # directory for storing logs\\n)\\n\\n# Create a Trainer instance\\ntrainer = Trainer(\\n    model=model,                     # the model to be trained\\n    args=training_args,              # TrainingArguments\\n    train_dataset=train_dataset,     # training dataset\\n    eval_dataset=eval_dataset        # evaluation dataset\\n)\\n\\n# Fine-tune the model\\ntrainer.train()\\n```\\n\\nAnd here is an example using Keras:\\n\\n```python\\nfrom tensorflow.keras.optimizers import Adam\\n\\n# Compile the model\\nmodel.compile(optimizer=Adam(lr=5e-5),   # optimizer with a learning rate of 5e-5\\n              loss='sparse_categorical_crossentropy',   # loss function\\n              metrics=['accuracy'])   # evaluation metric\\n\\n# Fine-tune the model\\nmodel.fit(train_dataset, epochs=3, batch_size=8, validation_data=eval_dataset)\\n```\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ask_question_from_webpage('write code for preprocessing')"
      ],
      "metadata": {
        "id": "iQSWjgddwqHl",
        "outputId": "8a0ae4c6-1daf-4fcf-b4b1-5a31bf86b5ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'import torch\\nfrom transformers import AutoTokenizer\\n\\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\\n\\ndef preprocess_data(data, max_length):\\n    tokenized_data = tokenizer(data, padding=\"max_length\", max_length=max_length, truncation=True, return_tensors=\"pt\")\\n    return tokenized_data\\n\\n# Example usage\\ndata = [\"This is an example sentence.\", \"Another example sentence.\"]\\nmax_length = 50\\nprocessed_data = preprocess_data(data, max_length)\\nprint(processed_data)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query='Quantum Telepathy Conquers an Unbeatable Game, what do you know this from there?'\n",
        "print(ask_question_from_webpage(url,query))"
      ],
      "metadata": {
        "id": "myRWedXIqzSB",
        "outputId": "0a7a394d-ed30-4398-ad08-efe289766689",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 4 is greater than number of elements in index 3, updating n_results = 3\n",
            "WARNING:chromadb.segment.impl.vector.local_hnsw:Number of requested results 5 is greater than number of elements in index 3, updating n_results = 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Based on the context provided, there is no information to suggest anything about Quantum Telepathy conquering an unbeatable game. The error message mentioned is related to a Varnish cache server issue and does not pertain to the topic of Quantum Telepathy conquering a game.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# crawling any website ... simple warmup"
      ],
      "metadata": {
        "id": "Z9cfbjQCign_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "def bfs_crawl(url, limit):\n",
        "    visited = set()\n",
        "    queue = [url]\n",
        "    count = 0\n",
        "    extracted_links = []\n",
        "\n",
        "    while queue and count < limit:\n",
        "        current_url = queue.pop(0)\n",
        "        visited.add(current_url)\n",
        "\n",
        "        try:\n",
        "            response = requests.get(current_url)\n",
        "        except requests.exceptions.RequestException:\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        for link in soup.find_all('a'):\n",
        "            href = link.get('href')\n",
        "            if href:\n",
        "                absolute_url = urljoin(current_url, href)\n",
        "                parsed_url = urlparse(absolute_url)\n",
        "\n",
        "                if parsed_url.netloc == urlparse(url).netloc:\n",
        "                    if absolute_url not in visited and absolute_url not in queue:\n",
        "                        queue.append(absolute_url)\n",
        "                        visited.add(absolute_url)\n",
        "                        count += 1\n",
        "                        extracted_links.append(absolute_url)\n",
        "\n",
        "        print(f\"Crawled: {current_url}\")\n",
        "\n",
        "    return extracted_links\n",
        "\n",
        "\n",
        "start_url = \"https://www.google.com/search?client=firefox-b-d&q=cat+image\"\n",
        "extraction_limit = 1000\n",
        "links = bfs_crawl(start_url, extraction_limit)\n",
        "print(links)"
      ],
      "metadata": {
        "id": "X-_X1e2hiaHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# extracting any data we want from any webpage"
      ],
      "metadata": {
        "id": "f3pHlMbuisNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import os\n",
        "import urllib.parse\n",
        "\n",
        "def extract_information(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    text_content = soup.get_text()\n",
        "\n",
        "    links = []\n",
        "    for link in soup.find_all('a'):\n",
        "        links.append(link.get('href'))    # if href doesn't have https or www , then use urljoin from urllib.parse\n",
        "\n",
        "    img_urls = []\n",
        "    for img in soup.find_all('img'):\n",
        "        img_url = img.get('src')\n",
        "        if img_url:\n",
        "            img_urls.append(urllib.parse.urljoin(url, img_url))\n",
        "\n",
        "    video_urls = []\n",
        "    for video in soup.find_all('video'):\n",
        "        video_url = video.get('src')\n",
        "        if video_url:\n",
        "            video_urls.append(urllib.parse.urljoin(url, video_url))\n",
        "\n",
        "    code_snippets = []\n",
        "    for code_block in soup.find_all('code'):\n",
        "        code_snippets.append(code_block.get_text())\n",
        "    return text_content, links, img_urls, video_urls, code_snippets\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qFcDz2VKinWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " parsing output (qdrant or something ) for text, code, mathematical equations, csv file or any file format ...\n",
        " then using different agents , toolkits , callbacks and others\n",
        "\n",
        " i can use STREAMLIT to build a simple web app or something...\n",
        " so i need to go through working on a project and finally deploy the complete pipeline ..."
      ],
      "metadata": {
        "id": "uDMP-SRlhTRM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# just from curiosity ____\n",
        "# dataset creation directly form any web page\n"
      ],
      "metadata": {
        "id": "wAjJlElZKfj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating custom data in pytorch custom dataset format\n",
        "# not only for paragragh tags, we can extract more tags , images , videos and lot more\n",
        "# It's kind of a way of generating data from any web site or other external sources\n",
        "\n",
        "import requests\n",
        "import langchain\n",
        "from bs4 import BeautifulSoup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class WebpageDataset(Dataset):\n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        self.texts = [p.get_text() for p in soup.find_all('p')]\n",
        "\n",
        "                                                                # img(src),video(src),code,div,p and more tags based on data format\n",
        "                                                                # we can extract from any web page using beautifulsoup ...\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx]\n",
        "\n",
        "def create_webpage_data_loader(url, batch_size=16, shuffle=True):\n",
        "    dataset = WebpageDataset(url)\n",
        "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "    return data_loader\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"http://some-website.com\"\n",
        "    batch_size = 16\n",
        "    shuffle = True\n",
        "    data_loader = create_webpage_data_loader(url, batch_size, shuffle)\n",
        "    for batch in data_loader:\n",
        "        print(batch)"
      ],
      "metadata": {
        "id": "VygJm5OOHU21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "loading code snippets probably:"
      ],
      "metadata": {
        "id": "LqEECHg7-uHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html_content =html\n",
        "\n",
        "# Parse the HTML content\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Find all code blocks\n",
        "code_blocks = soup.find_all('pre', class_='prism-code')\n",
        "\n",
        "# Extract code content from each code block\n",
        "code_snippets = []\n",
        "for code_block in code_blocks:\n",
        "    code_snippets.append(code_block.get_text())\n",
        "\n",
        "# Print the extracted code snippets\n",
        "for i, snippet in enumerate(code_snippets, start=1):\n",
        "    print(f\"Code snippet {i}:\")\n",
        "    print(snippet.strip())\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "A_n2-fIZ-sfG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}