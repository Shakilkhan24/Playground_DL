{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM4JGZH0ao9ft+IxERvtwG2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shakilkhan24/Playground_DL/blob/main/url_to_text_scrapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90fNpTMvQvg_"
      },
      "outputs": [],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "loader = WebBaseLoader([\"https://huggingface.co/docs/transformers/en/transformers_agents\"])\n",
        "docs = loader.load()\n"
      ],
      "metadata": {
        "id": "Xri6i1AhQ8hk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxAi4-LeRPtV",
        "outputId": "80251d62-e65f-4653-e95d-3711971efff0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Transformers Agents\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Hugging Face\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\t\t\t\t\tModels\n",
            "\n",
            "\t\t\t\t\tDatasets\n",
            "\n",
            "\t\t\t\t\tSpaces\n",
            "\n",
            "\t\t\t\t\tPosts\n",
            "\n",
            "\t\t\t\t\tDocs\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\t\t\tSolutions\n",
            "\t\t\n",
            "\n",
            "Pricing\n",
            "\t\t\t\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Log In\n",
            "\t\t\t\t\n",
            "Sign Up\n",
            "\t\t\t\t\t\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Transformers documentation\n",
            "\t\t\t\n",
            "Transformers Agents\n",
            "\n",
            "\n",
            "\n",
            "Transformers\n",
            "\n",
            "üè° View all docsAWS Trainium & InferentiaAccelerateAmazon SageMakerAutoTrainCompetitionsDatasetsDatasets-serverDiffusersEvaluateGradioHubHub Python LibraryHuggingface.jsInference API (serverless)Inference Endpoints (dedicated)OptimumPEFTSafetensorsTRLTasksText Embeddings InferenceText Generation InferenceTokenizersTransformersTransformers.jstimm\n",
            "\n",
            "Search documentation\n",
            "\n",
            "\n",
            "mainv4.39.0v4.38.2v4.37.2v4.36.1v4.35.2v4.34.1v4.33.3v4.32.1v4.31.0v4.30.0v4.29.1v4.28.1v4.27.2v4.26.1v4.25.1v4.24.0v4.23.1v4.22.2v4.21.3v4.20.1v4.19.4v4.18.0v4.17.0v4.16.2v4.15.0v4.14.1v4.13.0v4.12.5v4.11.3v4.10.1v4.9.2v4.8.2v4.7.0v4.6.0v4.5.1v4.4.2v4.3.3v4.2.2v4.1.1v4.0.1v3.5.1v3.4.0v3.3.1v3.2.0v3.1.0v3.0.2v2.11.0v2.10.0v2.9.1v2.8.0v2.7.0v2.6.0v2.5.1v2.4.1v2.3.0v2.2.2v2.1.1v2.0.0v1.2.0v1.1.0v1.0.0doc-builder-html\n",
            "DEENESFRHIITJAKOPTTETRZH\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Get started\n",
            "\n",
            "\n",
            "ü§ó Transformers\n",
            "Quick tour\n",
            "Installation\n",
            "\n",
            "\n",
            "Tutorials\n",
            "\n",
            "\n",
            "Run inference with pipelines\n",
            "Write portable code with AutoClass\n",
            "Preprocess data\n",
            "Fine-tune a pretrained model\n",
            "Train with a script\n",
            "Set up distributed training with ü§ó Accelerate\n",
            "Load and train adapters with ü§ó PEFT\n",
            "Share your model\n",
            "Agents\n",
            "Generation with LLMs\n",
            "\n",
            "\n",
            "Task Guides\n",
            "\n",
            "\n",
            "\n",
            "Natural Language Processing\n",
            "\n",
            "\n",
            "Audio\n",
            "\n",
            "\n",
            "Computer Vision\n",
            "\n",
            "\n",
            "Multimodal\n",
            "\n",
            "\n",
            "Generation\n",
            "\n",
            "\n",
            "Prompting\n",
            "\n",
            "\n",
            "\n",
            "Developer guides\n",
            "\n",
            "\n",
            "Use fast tokenizers from ü§ó Tokenizers\n",
            "Run inference with multilingual models\n",
            "Use model-specific APIs\n",
            "Share a custom model\n",
            "Templates for chat models\n",
            "Trainer\n",
            "Run training on Amazon SageMaker\n",
            "Export to ONNX\n",
            "Export to TFLite\n",
            "Export to TorchScript\n",
            "Benchmarks\n",
            "Notebooks with examples\n",
            "Community resources\n",
            "Custom Tools and Prompts\n",
            "Troubleshoot\n",
            "Contribute new quantization method\n",
            "\n",
            "\n",
            "Performance and scalability\n",
            "\n",
            "\n",
            "Overview\n",
            "Quantization\n",
            "\n",
            "Efficient training techniques\n",
            "\n",
            "\n",
            "Methods and tools for efficient training on a single GPU\n",
            "Multiple GPUs and parallelism\n",
            "Fully Sharded Data Parallel\n",
            "DeepSpeed\n",
            "Efficient training on CPU\n",
            "Distributed CPU training\n",
            "Training on TPU with TensorFlow\n",
            "PyTorch training on Apple silicon\n",
            "Custom hardware for training\n",
            "Hyperparameter Search using Trainer API\n",
            "\n",
            "\n",
            "Optimizing inference\n",
            "\n",
            "\n",
            "CPU inference\n",
            "GPU inference\n",
            "\n",
            "Instantiating a big model\n",
            "Debugging\n",
            "XLA Integration for TensorFlow Models\n",
            "Optimize inference using `torch.compile()`\n",
            "\n",
            "\n",
            "Contribute\n",
            "\n",
            "\n",
            "How to contribute to ü§ó Transformers?\n",
            "How to add a model to ü§ó Transformers?\n",
            "How to convert a ü§ó Transformers model to TensorFlow?\n",
            "How to add a pipeline to ü§ó Transformers?\n",
            "Testing\n",
            "Checks on a Pull Request\n",
            "\n",
            "\n",
            "Conceptual guides\n",
            "\n",
            "\n",
            "Philosophy\n",
            "Glossary\n",
            "What ü§ó Transformers can do\n",
            "How ü§ó Transformers solve tasks\n",
            "The Transformer model family\n",
            "Summary of the tokenizers\n",
            "Attention mechanisms\n",
            "Padding and truncation\n",
            "BERTology\n",
            "Perplexity of fixed-length models\n",
            "Pipelines for webserver inference\n",
            "Model training anatomy\n",
            "Getting the most out of LLMs\n",
            "\n",
            "\n",
            "API\n",
            "\n",
            "\n",
            "\n",
            "Main Classes\n",
            "\n",
            "\n",
            "Agents and Tools\n",
            "Auto Classes\n",
            "Backbones\n",
            "Callbacks\n",
            "Configuration\n",
            "Data Collator\n",
            "Keras callbacks\n",
            "Logging\n",
            "Models\n",
            "Text Generation\n",
            "ONNX\n",
            "Optimization\n",
            "Model outputs\n",
            "Pipelines\n",
            "Processors\n",
            "Quantization\n",
            "Tokenizer\n",
            "Trainer\n",
            "DeepSpeed\n",
            "Feature Extractor\n",
            "Image Processor\n",
            "\n",
            "\n",
            "Models\n",
            "\n",
            "\n",
            "\n",
            "Text models\n",
            "\n",
            "\n",
            "Vision models\n",
            "\n",
            "\n",
            "Audio models\n",
            "\n",
            "\n",
            "Video models\n",
            "\n",
            "\n",
            "Multimodal models\n",
            "\n",
            "\n",
            "Reinforcement learning models\n",
            "\n",
            "\n",
            "Time series models\n",
            "\n",
            "\n",
            "Graph models\n",
            "\n",
            "\n",
            "\n",
            "Internal Helpers\n",
            "\n",
            "\n",
            "Custom Layers and Utilities\n",
            "Utilities for pipelines\n",
            "Utilities for Tokenizers\n",
            "Utilities for Trainer\n",
            "Utilities for Generation\n",
            "Utilities for Image Processors\n",
            "Utilities for Audio processing\n",
            "General Utilities\n",
            "Utilities for Time Series\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Join the Hugging Face community\n",
            "and get access to the augmented documentation experience\n",
            "\t\t\n",
            "\n",
            "Collaborate on models, datasets and Spaces\n",
            "\t\t\t\t\n",
            "\n",
            "Faster examples with accelerated inference\n",
            "\t\t\t\t\n",
            "\n",
            "Switch between documentation themes\n",
            "\t\t\t\t\n",
            "Sign Up\n",
            "to get started\n",
            " \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "   Transformers Agents Transformers Agents is an experimental API which is subject to change at any time. Results returned by the agents\n",
            "can vary as the APIs or underlying models are prone to change. Transformers version v4.29.0, building on the concept of tools and agents. You can play with in\n",
            "this colab. In short, it provides a natural language API on top of transformers: we define a set of curated tools and design an\n",
            "agent to interpret natural language and to use these tools. It is extensible by design; we curated some relevant tools,\n",
            "but we‚Äôll show you how the system can be extended easily to use any tool developed by the community. Let‚Äôs start with a few examples of what can be achieved with this new API. It is particularly powerful when it comes\n",
            "to multimodal tasks, so let‚Äôs take it for a spin to generate images and read text out loud.   Copied agent.run(\"Caption the following image\", image=image) Input Output  A beaver is swimming in the water    Copied agent.run(\"Read the following text out loud\", text=text) Input Output A beaver is swimming in the water  your browser does not support the audio element.    Copied agent.run(\n",
            "    \"In the following `document`, where will the TRRF Scientific Advisory Council Meeting take place?\",\n",
            "    document=document,\n",
            ") Input Output  ballroom foyer  Quickstart Before being able to use agent.run, you will need to instantiate an agent, which is a large language model (LLM).\n",
            "We provide support for openAI models as well as opensource alternatives from BigCode and OpenAssistant. The openAI\n",
            "models perform better (but require you to have an openAI API key, so cannot be used for free); Hugging Face is\n",
            "providing free access to endpoints for BigCode and OpenAssistant models. To start with, please install the agents extras in order to install all default dependencies.   Copied pip install transformers[agents] To use openAI models, you instantiate an OpenAiAgent after installing the openai dependency:   Copied pip install openai   Copied from transformers import OpenAiAgent\n",
            "\n",
            "agent = OpenAiAgent(model=\"text-davinci-003\", api_key=\"<your_api_key>\") To use BigCode or OpenAssistant, start by logging in to have access to the Inference API:   Copied from huggingface_hub import login\n",
            "\n",
            "login(\"<YOUR_TOKEN>\") Then, instantiate the agent   Copied from transformers import HfAgent\n",
            "\n",
            "# Starcoder\n",
            "agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoder\")\n",
            "# StarcoderBase\n",
            "# agent = HfAgent(\"https://api-inference.huggingface.co/models/bigcode/starcoderbase\")\n",
            "# OpenAssistant\n",
            "# agent = HfAgent(url_endpoint=\"https://api-inference.huggingface.co/models/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\") This is using the inference API that Hugging Face provides for free at the moment. If you have your own inference\n",
            "endpoint for this model (or another one) you can replace the URL above with your URL endpoint. StarCoder and OpenAssistant are free to use and perform admirably well on simple tasks. However, the checkpoints\n",
            "don‚Äôt hold up when handling more complex prompts. If you‚Äôre facing such an issue, we recommend trying out the OpenAI\n",
            "model which, while sadly not open-source, performs better at this given time. You‚Äôre now good to go! Let‚Äôs dive into the two APIs that you now have at your disposal.  Single execution (run) The single execution method is when using the run() method of the agent:   Copied agent.run(\"Draw me a picture of rivers and lakes.\")  It automatically selects the tool (or tools) appropriate for the task you want to perform and runs them appropriately. It\n",
            "can perform one or several tasks in the same instruction (though the more complex your instruction, the more likely\n",
            "the agent is to fail).   Copied agent.run(\"Draw me a picture of the sea then transform the picture to add an island\")   Every run() operation is independent, so you can run it several times in a row with different tasks. Note that your agent is just a large-language model, so small variations in your prompt might yield completely\n",
            "different results. It‚Äôs important to explain as clearly as possible the task you want to perform. We go more in-depth\n",
            "on how to write good prompts here. If you‚Äôd like to keep a state across executions or to pass non-text objects to the agent, you can do so by specifying\n",
            "variables that you would like the agent to use. For example, you could generate the first image of rivers and lakes,\n",
            "and ask the model to update that picture to add an island by doing the following:   Copied picture = agent.run(\"Generate a picture of rivers and lakes.\")\n",
            "updated_picture = agent.run(\"Transform the image in `picture` to add an island to it.\", picture=picture) This can be helpful when the model is unable to understand your request and mixes tools. An example would be:   Copied agent.run(\"Draw me the picture of a capybara swimming in the sea\") Here, the model could interpret in two ways: Have the text-to-image generate a capybara swimming in the sea Or, have the text-to-image generate capybara, then use the image-transformation tool to have it swim in the sea In case you would like to force the first scenario, you could do so by passing it the prompt as an argument:   Copied agent.run(\"Draw me a picture of the `prompt`\", prompt=\"a capybara swimming in the sea\")  Chat-based execution (chat) The agent also has a chat-based approach, using the chat() method:   Copied agent.chat(\"Generate a picture of rivers and lakes\")    Copied agent.chat(\"Transform the picture so that there is a rock in there\")   This is an interesting approach when you want to keep the state across instructions. It‚Äôs better for experimentation,\n",
            "but will tend to be much better at single instructions rather than complex instructions (which the run()\n",
            "method is better at handling). This method can also take arguments if you would like to pass non-text types or specific prompts.  ‚ö†Ô∏è Remote execution For demonstration purposes and so that it could be used with all setups, we had created remote executors for several\n",
            "of the default tools the agent has access for the release. These are created using\n",
            "inference endpoints. We have turned these off for now, but in order to see how to set up remote executors tools yourself,\n",
            "we recommend reading the custom tool guide.  What‚Äôs happening here? What are tools, and what are agents?   Agents The ‚Äúagent‚Äù here is a large language model, and we‚Äôre prompting it so that it has access to a specific set of tools. LLMs are pretty good at generating small samples of code, so this API takes advantage of that by prompting the\n",
            "LLM gives a small sample of code performing a task with a set of tools. This prompt is then completed by the\n",
            "task you give your agent and the description of the tools you give it. This way it gets access to the doc of the\n",
            "tools you are using, especially their expected inputs and outputs, and can generate the relevant code.  Tools Tools are very simple: they‚Äôre a single function, with a name, and a description. We then use these tools‚Äô descriptions\n",
            "to prompt the agent. Through the prompt, we show the agent how it would leverage tools to perform what was\n",
            "requested in the query. This is using brand-new tools and not pipelines, because the agent writes better code with very atomic tools.\n",
            "Pipelines are more refactored and often combine several tasks in one. Tools are meant to be focused on\n",
            "one very simple task only.  Code-execution?! This code is then executed with our small Python interpreter on the set of inputs passed along with your tools.\n",
            "We hear you screaming ‚ÄúArbitrary code execution!‚Äù in the back, but let us explain why that is not the case. The only functions that can be called are the tools you provided and the print function, so you‚Äôre already\n",
            "limited in what can be executed. You should be safe if it‚Äôs limited to Hugging Face tools. Then, we don‚Äôt allow any attribute lookup or imports (which shouldn‚Äôt be needed anyway for passing along\n",
            "inputs/outputs to a small set of functions) so all the most obvious attacks (and you‚Äôd need to prompt the LLM\n",
            "to output them anyway) shouldn‚Äôt be an issue. If you want to be on the super safe side, you can execute the\n",
            "run() method with the additional argument return_code=True, in which case the agent will just return the code\n",
            "to execute and you can decide whether to do it or not. The execution will stop at any line trying to perform an illegal operation or if there is a regular Python error\n",
            "with the code generated by the agent.  A curated set of tools We identify a set of tools that can empower such agents. Here is an updated list of the tools we have integrated\n",
            "in transformers: Document question answering: given a document (such as a PDF) in image format, answer a question on this document (Donut) Text question answering: given a long text and a question, answer the question in the text (Flan-T5) Unconditional image captioning: Caption the image! (BLIP) Image question answering: given an image, answer a question on this image (VILT) Image segmentation: given an image and a prompt, output the segmentation mask of that prompt (CLIPSeg) Speech to text: given an audio recording of a person talking, transcribe the speech into text (Whisper) Text to speech: convert text to speech (SpeechT5) Zero-shot text classification: given a text and a list of labels, identify to which label the text corresponds the most (BART) Text summarization: summarize a long text in one or a few sentences (BART) Translation: translate the text into a given language (NLLB) These tools have an integration in transformers, and can be used manually as well, for example:   Copied from transformers import load_tool\n",
            "\n",
            "tool = load_tool(\"text-to-speech\")\n",
            "audio = tool(\"This is a text to speech tool\")  Custom tools While we identify a curated set of tools, we strongly believe that the main value provided by this implementation is\n",
            "the ability to quickly create and share custom tools. By pushing the code of a tool to a Hugging Face Space or a model repository, you‚Äôre then able to leverage the tool\n",
            "directly with the agent. We‚Äôve added a few\n",
            "transformers-agnostic tools to the huggingface-tools organization: Text downloader: to download a text from a web URL Text to image: generate an image according to a prompt, leveraging stable diffusion Image transformation: modify an image given an initial image and a prompt, leveraging instruct pix2pix stable diffusion Text to video: generate a small video according to a prompt, leveraging damo-vilab The text-to-image tool we have been using since the beginning is a remote tool that lives in\n",
            "huggingface-tools/text-to-image! We will\n",
            "continue releasing such tools on this and other organizations, to further supercharge this implementation. The agents have by default access to tools that reside on huggingface-tools.\n",
            "We explain how to you can write and share your tools as well as leverage any custom tool that resides on the Hub in following guide.  Code generation So far we have shown how to use the agents to perform actions for you. However, the agent is only generating code\n",
            "that we then execute using a very restricted Python interpreter. In case you would like to use the code generated in\n",
            "a different setting, the agent can be prompted to return the code, along with tool definition and accurate imports. For example, the following instruction   Copied agent.run(\"Draw me a picture of rivers and lakes\", return_code=True) returns the following code   Copied from transformers import load_tool\n",
            "\n",
            "image_generator = load_tool(\"huggingface-tools/text-to-image\")\n",
            "\n",
            "image = image_generator(prompt=\"rivers and lakes\") that you can then modify and execute yourself. \n",
            "\n",
            "\n",
            "‚ÜêShare your model\n",
            "Generation with LLMs‚Üí\n",
            "\n",
            "\n",
            "Transformers Agents\n",
            "Quickstart\n",
            "Single execution (run)\n",
            "Chat-based execution (chat)\n",
            "‚ö†Ô∏è Remote execution\n",
            "What‚Äôs happening here? What are tools, and what are agents?\n",
            "Agents\n",
            "Tools\n",
            "Code-execution?!\n",
            "A curated set of tools\n",
            "Custom tools\n",
            "Code generation\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# give the extracted text to any llm chatbot, and ask about anything from the text or extracted data ... for example (claude)"
      ],
      "metadata": {
        "id": "tbQR1DS8eTOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FANq-ojXen4m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}